Dear stakeholder,

I have checked the three data files that are receipts data, users data and brands data. After my evaluation, I found that there are some quality issues including duplicate data, unnecessary data format and too many missing values. I used Python to open each data and found that some data such as user ID are stored as nested dictionary which is unnecessary and takes more spaces to store. I also checked if the same data is entered multiple times and found out that 71% of users show up more than one time. And Many of our variables are missing some data, for example, we don’t know the category code for 13% of brands. Therefore I need to know how did we collect the data, in particular, why are duplicate user data entered multiple times, and I would like to know how the data was processed before it came to us? So that I can understand the way to either fix the data source system or improve the data pipeline to resolve our data quality problem.  

What’s more, I would like to know the traffic of our platform so that I can ensure the database server I’m trying to create can handle the expected workload. We also need to be prepared to use more advanced or stronger hardware such as disk space or a faster central processing unit (CPU) To deal with the problem of decreased performance caused by the increased workload. I can also use NoSQL databases to hand workload problems since they tend to be more scalable than SQL databases. Thank you for reading and looking forward to your feedback.

Best regards,  
Feibai pan

